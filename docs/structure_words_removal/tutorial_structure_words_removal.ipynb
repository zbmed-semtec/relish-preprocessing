{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure words removal\n",
    "\n",
    "The following script is used to generate a list of common structure words that appear in the articles' abstract. These structure words are words or groups of words followed by a colon, most likely required by the journals. Since they do not provide any meaningful information to the text, they should be removed.\n",
    "\n",
    "Some examples of these structure words are \"BACKGROUND:\", \"CONCLUSIONS:\", \"METHODS:\", etc.\n",
    "\n",
    "The regular expression that matches these structure words follows:\n",
    "\n",
    "* The word/group of words should be at least 3 characters long and at most 70.\n",
    "\n",
    "* The word/group of words must be at the start of a sentence.\n",
    "\n",
    "* The word/group of words must be followed by a colon and an empty space \": \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. A TSV file which contains an abstract column. The file can be generated following instructions [here](https://github.com/zbmed-semtec/medline-preprocessing/blob/main/docs/BioC_API_Tutorials/tutorial_RELISH.ipynb)\n",
    "2. (Optional) A list of structure words. If you already have one, you can skip steps 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "## Step 1: Imports\n",
    "\n",
    "Import the necessary packages and files to run the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "sys.path.append('../../code/Structure_Words_removal/')\n",
    "\n",
    "import structurewords_remover as SW_rem\n",
    "import structurewords_list_generator as SW_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading the data (Optional)\n",
    "\n",
    "Load the input `.TSV` files from which the structure words list will be generated. You can either input a file itself or a directory containing several files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_files = [\"../../data/RELISH/RELISH_documents.tsv\",\n",
    "            \"../../data/TREC/TREC_documents.tsv\"]\n",
    "\n",
    "input_dir = \"../../data/\"\n",
    "in_files = glob.glob(f\"{input_dir}/*.tsv\")\n",
    "\n",
    "df_list = []\n",
    "for file in in_files:\n",
    "    df_list.append(pd.read_csv(file, sep=\"\\t\", quotechar=\"`\"))\n",
    "data = pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate the list (Optional)\n",
    "\n",
    "Generate the structure word list. In order to avoid false positives from the algorithm (removing words that can be relevant but follow the regular expression), it is advised to input either a minimum relative frequency of appearance or a minimum number of total appearances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_words = SW_lg.structure_words_pipeline(\n",
    "    data, ratio_threshold=0, occurrences_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list can be pruned afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of structure words matched: 10362\n",
      "Total number of structure words after pruning: 241\n"
     ]
    }
   ],
   "source": [
    "structure_words_pruned = SW_lg.prune_structure_words(\n",
    "    structure_words, ratio_threshold=0.0001, occurrences_threshold=10)\n",
    "\n",
    "print(f\"Total number of structure words matched: {len(structure_words)}\")\n",
    "print(\n",
    "    f\"Total number of structure words after pruning: {len(structure_words_pruned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list can be stored either as plain words or with some extra information like the relative frequency and the total occurrences. It is recommended to manually check the list in order to identify if some false positives made through the cut, but with a reasonable threshold, the effects of a few false positives should be negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW_lg.export_to_list(\"structure_word_list.txt\", structure_words_pruned)\n",
    "SW_lg.export_to_list(\"../../data/Structure_Words_removal/structure_word_list_pruned.txt\", structure_words_pruned)\n",
    "SW_lg.export_to_json(\"../../data/Structure_Words_removal/structure_word_list.json\", structure_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load the list\n",
    "\n",
    "Once you are sure that the list of structure words are ready to be removed from the abstract, you can either read the list from the file or convert the current variable to a suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_words_list = SW_rem.read_list(\"structure_word_list.txt\")\n",
    "\n",
    "#structure_words_list = SW_lg.convert_to_list(structure_words_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the loaded list is in `.json` format and have the extra information of the structure words, the list can be pruned afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_words = SW_rem.read_json(\"../../data/Structure_Words_removal/structure_word_list.json\")\n",
    "\n",
    "structure_words_pruned = SW_lg.prune_structure_words(structure_words, ratio_threshold=0.0001)\n",
    "structure_words_list = SW_lg.convert_to_list(structure_words_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Removing the structure words\n",
    "\n",
    "Once the list of structure words is ready, you can remove them from the abstract by loading a `.tsv` file and running `structure_words_remover()`. Save the data in the desired output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../../data/RELISH/RELISH_documents.tsv\"\n",
    "output_file = \"../../data/RELISH/RELISH_documents_pruned.tsv\"\n",
    "\n",
    "data = pd.read_csv(input_file, sep=\"\\t\", quotechar=\"`\")\n",
    "\n",
    "data_pruned = SW_rem.structure_words_remover(data, structure_words_list)\n",
    "\n",
    "data_pruned.to_csv(output_file, sep=\"\\t\", quotechar=\"`\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to remove structure words from XML files. Given a directory path, two lists containing the input and output files are created. You can the run the `pipeline_xml` function to remove the structure words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import glob\n",
    "import os\n",
    "\n",
    "input_path = \"../../data/RELISH/xml-files/pmid-xml/\"\n",
    "output_path = \"../../data/RELISH/xml-files/pmid-xml_no_structure_words/\"\n",
    "\n",
    "if not os.path.isdir(output_path): os.mkdir(output_path)\n",
    "\n",
    "# It is a directory:\n",
    "input_files = glob.glob(input_path + \"/*.xml\")\n",
    "output_files = list(map(lambda x: x.replace(input_path, output_path), input_files))\n",
    "\n",
    "SW_rem.pipeline_xml(input_files, output_files, structure_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output XML files are in the  `data/RELISH/xml-files/pmid-xml_no_structure_words` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other options\n",
    "\n",
    "## Generate the list from the script\n",
    "\n",
    "The code files are prepared to work on their own given some parameters. In order to execute the structure words list generator script, run the following command:\n",
    "\n",
    "```bash\n",
    "python structurewords_list_generator.py [-h] (-i INPUT | -d INDIR) [-o OUTPUT] [--ratio_threshold RATIO_THRESHOLD] [--occurrences_threshold OCCURRENCES_THRESHOLD]\n",
    "```\n",
    "\n",
    "You must pass one of the following arguments:\n",
    "\n",
    "* -i / --input: path to TSV file with the data.\n",
    "\n",
    "* -d / --indir: path to directory containing the TSV files with the data.\n",
    "\n",
    "Optionally, you can pass the following arguments:\n",
    "\n",
    "* -o / --output: path to the output structure words list.\n",
    "\n",
    "* --ratio_threshold: minimum relative frequency of appearance to be saved.\n",
    "\n",
    "* --occurrence_threshold: minimum number of total appearances to be saved.\n",
    "\n",
    "An example of the command that will create the files `structure_word_list.txt`, `structure_word_list.json` (one containing the list itself and the other containing other relevant information of the structure words selected):\n",
    "\n",
    "```bash\n",
    "python structurewords_list_generator.py --input ../../data/RELISH/RELISH_documents.tsv --ratio_threshold 0.0001\n",
    "```\n",
    "\n",
    "## Remove the structure words from the script\n",
    "\n",
    "The code files are prepared to work on their own given some parameters. In order to execute the structure words removal script, run the following command:\n",
    "\n",
    "```bash\n",
    "python structurewords_remover.py [-h] [-i INPUT | -d INDIR] -l LIST [-o OUTPUT]\n",
    "```\n",
    "\n",
    "You must pass one of the following arguments:\n",
    "\n",
    "* -i / --input: path to TSV file with the data.\n",
    "\n",
    "* -d / --indir: path to directory containing the TSV files with the data.\n",
    "\n",
    "Optionally, you can pass the following arguments:\n",
    "\n",
    "* -o / --output: path to the output pruned file.\n",
    "\n",
    "* -l / --list: path to the file generated from the structurewords_list_generator script that contains the structure word list.\n",
    "\n",
    "An example of the command that will create a pruned `.tsv`:\n",
    "\n",
    "```bash\n",
    "python structurewords_remover.py --input ../../data/RELISH/RELISH_documents.tsv --list structure_word_list.txt --output RELISH_documents_pruned.tsv\n",
    "```\n",
    "\n",
    "The script can also remove structure words from XML files, however, it is recommended to input `.tsv` files with a dedicated `abstract` column per publication. In order to execute the script for XML files, input a file or a directory containing XML files, a output file/directory (defaults to file/directory name plus `_no_structure_words`) and set the `--xml` tag to 1.\n",
    "\n",
    "```bash\n",
    "python structurewords_remover.py --indir ../../data/RELISH/xml-files/pmid-xml/ --list ../../data/Structure_Words_removal/structure_word_list_prunned.txt\n",
    "```\n",
    "\n",
    "The output XML files are in the  `data/RELISH/xml-files/pmid-xml_no_structure_words` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Example of structure words removal:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Abstract Input</th>\n",
    "<th>Abstract Output</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<mark>OBJECTIVE: </mark>To describe the development of evidence-based electronic prescribing (e-prescribing) triggers and treatment algorithms for potentially inappropriate medications (PIMs) for older adults. <mark>DESIGN: </mark>Literature review, expert panel and focus group. <mark>SETTING: </mark>Primary care with access to e-prescribing systems. <mark>PARTICIPANTS: </mark>Primary care physicians using e-prescribing systems receiving medication history. <mark>INTERVENTIONS: </mark>Standardised treatment algorithms for clinicians attempting to prescribe PIMs for older patients. <mark>MAIN OUTCOME MEASURE: </mark>Development of 15 treatment algorithms suggesting alternative therapies. <mark>RESULTS: </mark>Evidence-based treatment algorithms were well received by primary care physicians. Providing alternatives to PIMs would make it easier for physicians to change decisions at the point of prescribing. <mark>CONCLUSION: </mark>Prospectively identifying older persons receiving PIMs or with adherence issues and providing feasible interventions may prevent adverse drug events.\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "To describe the development of evidence-based electronic prescribing (e-prescribing) triggers and treatment algorithms for potentially inappropriate medications (PIMs) for older adults. Literature review, expert panel and focus group. Primary care with access to e-prescribing systems. Primary care physicians using e-prescribing systems receiving medication history. Standardised treatment algorithms for clinicians attempting to prescribe PIMs for older patients. Development of 15 treatment algorithms suggesting alternative therapies. Evidence-based treatment algorithms were well received by primary care physicians. Providing alternatives to PIMs would make it easier for physicians to change decisions at the point of prescribing. Prospectively identifying older persons receiving PIMs or with adherence issues and providing feasible interventions may prevent adverse drug events.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision notes\n",
    "\n",
    "## Code strategy:\n",
    "\n",
    "1. Using a regular expression (`r\"(?:^[\\s]*|\\.|\\?|: )(?: *)(?=([A-Z]+[A-Z\\s&a-z]{2,69}:\\s))\"`), find all matches in the input abstracts. Add the matches to a dictionary and keep track of how many times each match is found.\n",
    "\n",
    "2. The dictionary with `{word: counts}` is converted into a list of dictionaries that stores the word, the total occurrences, and the relative frequency (total occurrences divided by total number of articles): `[{word: _, occurrences: _, ratio_of_occurrences: occurrences/len(data)}, ...]`\n",
    "\n",
    "3. The list is pruned by both the total occurrences or the relative frequency. Thus, most words that only appear a few times will not be removed in order to avoid false positives. A recommended minimum relative frequency is 0.0001.\n",
    "\n",
    "4. Save the list into a text format document to be read by the remover program and to allow humans to manually inspect it.\n",
    "\n",
    "5. To remove the structure words from the abstract:\n",
    "\n",
    "    5.1 The first step is to divide the structure word list in those that match the regular expression and those that not. If the user has added custom structure words, they might not follow the pattern.\n",
    "\n",
    "    5.2 Loop through every abstract and match the same regular expression used to generate the structure words.\n",
    "\n",
    "    5.3 For every match, check if the word is in the structure words removal list, and save the starting and ending position of the match.\n",
    "\n",
    "    5.4 The last step is to extract the characters contained between the starting and ending position of each match, leaving behind the abstract without the desired structure words.\n",
    "\n",
    "    5.5 For every structure word in the non-matched list, remove it from the abstract. We cannot use a regular expression in this case.\n",
    "\n",
    "6. Save the pruned file into a `.tsv`.\n",
    "\n",
    "## Decisions:\n",
    "\n",
    "* Why to split the procedure in \"generating the structure word list\" and \"removing the structure words\"? \n",
    "\n",
    "    To generate the list, the user has to input as many articles as possible in order to have a more complete list. Once the list is generated, if new articles are added to the pipeline, it is not necessary to run the list generator algorithm again. Thus, I believe that splitting the two processes allows for a faster deployment of new data.\n",
    "\n",
    "* Why is this process not integrated in the pre-process pipeline, like lower case everything or stopword removal?\n",
    "\n",
    "    Most of the other pre-process techniques requires previous tokenization or splitting of words. This is different. The structure words require both the period and colon punctuations to identify them, and some structure words can be composed of several words (like \"MAIN OUTCOME MEASURES: \"). I think the process is different enough for it to have each own pipeline.\n",
    "\n",
    "    Also, this process should be common in every approach (including [whatizit-dictionary-ner](https://github.com/zbmed-semtec/whatizit-dictionary-ner)), so creating a modified `.tsv` file facilitates its integration.\n",
    "\n",
    "* In the algorithm to remove the structure words, why to split the structure words in those that matched the regular expression and those that don't?\n",
    "\n",
    "    For efficiency. Most of the structure words are going to follow the regular expression, and removing them using the regular expression is quite efficient. I tested the use of the `replace()` function for everything, but it had a huge performance impact. For the few custom structure words that might be added, using the `replace()` function is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE THIS LINE BEFORE FINAL VERSION COMMIT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tutorial_structure_words_removal.ipynb to markdown\n",
      "[NbConvertApp] Writing 14219 bytes to README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert tutorial_structure_words_removal.ipynb --to markdown --output README.md"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "902ab70f0cd0f11189f4f4c93e73d750b8b25ffc48666e69e012efe9922b8b8f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ZB-MED')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
